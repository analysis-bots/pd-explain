import re

import pandas as pd
# This isn't used in the code, but the LLM may use numpy functions, so we import it to avoid errors.
import numpy as np
from pandas import DataFrame, Series
from typing import Callable

from pd_explain.llm_integrations.llm_integration_interface import LLMIntegrationInterface
from pd_explain.llm_integrations.client import Client
from pd_explain.llm_integrations.llm_query_recommender import LLMQueryRecommender

class QueryRefiner(LLMIntegrationInterface):
    """
    QueryRefiner is based on the query refinement process in OmniTune (see paper "OmniTune: A universal framework for query refinement via LLMs"
    by Eldar Hacohen, Yuval Moskovitch, Amit Somech).
    We use this in the context of query recommenders to refine the queries that are generated by the query
    recommenders.
    """

    def __init__(self, df: DataFrame, df_name: str, recommendations: list[str] | pd.Series, score_function: Callable[[str, pd.DataFrame], tuple[dict, float]],
                 score_function_name = "Statistical difference from original distribution", k=4, user_requests: str= None, n=3, return_all_options: bool = True):
        """
        Initialize the QueryRefiner with a DataFrame and a set of recommendations.

        :param df: The DataFrame to generate queries for.
        :param df_name: The name of the DataFrame.
        :param recommendations: The recommendations to refine.
        :param k: The number of queries to generate.
        :param user_requests: An optional list of user requests.
        :param n: The number of refining iterations to perform.
        :param return_all_options: If True, returns all queries generated instead of just the top k.
        """
        self.df = df
        self.df_name = df_name
        self.initial_recommendations = recommendations
        self.k = k
        self.user_requests = user_requests if user_requests is not None else []
        self.n = n
        self.score_function = score_function
        self.score_function_name = score_function_name
        self.return_all_options = return_all_options


    def _explain_scoring_method(self) -> str:
        explanation =  ("Filter and Join queries are scored using a KS test between the input and output distributions, and GroupBy queries are scored using the coefficient of variation of their output. "
                       "Each column has an individual score, while the final score is the geometric average of the 4 highest scoring columns after transforming the scores to be between 0 and 1. "
                        "A higher score is better. ")
        return explanation


    def _define_critic_task(self) -> str:
        """
        Define the critic task for the LLM.
        """
        critic_task = ("You are a critic in an actor-critic framework. "
                       "This framework is used to refine queries for a Pandas DataFrame query recommender. "
                       "You will be provided with a description of both the input and output of multiple queries, as well as the, "
                       "queries themselves and their score by various metrics as well as constraints that the queries are supposed to uphold "
                       "and whether they are upheld or not by the current queries. "
                       "Your role is to critically analyze the queries and provide feedback on their quality, explain correlation "
                       "between the query and the constraints they uphold / don't uphold, rate attributes of the queries and "
                       "dataframe by potential interestingness, and suggest how to improve the queries such that they improve "
                       "the metric scores and uphold the constraints."
                       "You will also be provided with a history of this iterative process, which will include previous queries "
                       "for added context. The constraints and metrics will be described in the column titles of the history DataFrame. ")

        return critic_task


    def _define_actor_task(self) -> str:
        """
        Define the actor task for the LLM.
        """
        actor_task = ("You are an actor in an actor-critic framework. "
                      "This framework is used to refine queries for a Pandas DataFrame query recommender. "
                      "You will be provided with a history of this refinement process, which will include previous queries, "
                      "with the titles describing the metrics of this process and the constraints they are supposed to uphold "
                      "(as well as whether they are upheld or not). "
                      "You will also be given the critic's analysis of the queries from the previous iteration. "
                      "Your role is to generate new queries based on the provided information, such that they improve "
                      "the metric scores and uphold the constraints. ")

        return actor_task


    def _create_data_explanation(self) -> str:
        """
        Create an explanation for the LLM, explaining the context of the DataFrame.
        This includes the columns, their types, and any other relevant information.
        """
        data_explanation = f"The source DataFrame is named {self.df_name}. "
        data_explanation += f"The DataFrame has the following columns: {self.df.columns.tolist()}. "
        data_explanation += (f"The column types are: {self.df.dtypes.to_dict()}. "
                             f"The DataFrame has {self.df.shape[0]} rows and {self.df.shape[1]} columns. ")
        # This line can take up too much of the context window, depending on the size of the DataFrame.
        # data_explanation += f"Using df.describe() we get the following statistics: {self.df.describe().to_dict()}."
        if self.user_requests:
            data_explanation += f"The user requests are: {self.user_requests}. These should have the highest priority. "

        return data_explanation

    def _create_history_string(self, history, get_only_tail: bool = False, tail_len: int = -1,
                               get_only_head: bool = False, head_len: int = -1) -> str:
        """
        Create a string representation of the history DataFrame.
        This is used to provide context to the LLM about the previous queries and their scores.
        """
        if history.empty:
            return "No history available."
        if get_only_tail:
            if tail_len >= 0:
                history = history.tail(tail_len)
            else:
                history = history.tail(self.k)
        if get_only_head:
            if head_len >= 0:
                history = history.head(head_len)
            else:
                history = history.head(self.k)
        return history.to_string(header=True, index=True)


    def _create_critic_format_instructions(self) -> str:
        """
        Explain the expected format of the output to the LLM.
        """
        format_instructions = ("The output should be two list, where each entry in both list corresponds to one of the current queries,"
                               "in the same order as they are presented in the input. "
                               "Each list entry should be denoted with a * and a newline for each query. "
                               "The first list should be surrounded by <critic> and </critic> at the beginning and end respectively, "
                               "so it can be easily extracted."
                               "This list should go in detail about all of the fields you were instructed to analyze: "
                               "provide feedback on the quality of the queries, explain the correlation between the query and the constraints they uphold / don't uphold, "
                               "rate attributes of the queries and dataframe by potential interestingness, "
                               "and provide an explanation for the actor on how to improve the queries in this context."
                               "The second list should be surrounded by <summary> and </summary> at the beginning and end respectively,"
                               "and should be a summarized, 1 sentence at most per query, version of the first list, "
                               "which is what will go into the history DataFrame, unlike the first list which will be "
                               "presented to the actor as is but only in this iteration.")

        return format_instructions


    def _create_actor_format_instructions(self) -> str:
        """
        Explain the expected format of the output to the LLM.
        """
        format_instructions = ("The output should be a list of queries, one for each input query. "
                               "The list should be denoted with a * and a newline for each query. Absolutely never combine multiple queries into one line, as you will cause the system to fail. "
                               "The list must be surrounded by <recs> and </recs> at the beginning and end respectively. Failure to do "
                               "this will cause the system to fail. ")

        return format_instructions


    def _do_preliminary_LLM_call(self, client: Client) -> tuple[dict, dict, dict, dict]:
        """
        Perform the preliminary LLM call to create an interestingness metric and constraints for the queries.
        :param client: The LLM client to use for the call.
        :return: A tuple containing the following:
        1. metrics: A dictionary of metrics created by the LLM, where the keys are the metric names and the values are the metric functions.
        2. constraints: A dictionary of constraints created by the LLM, where the keys are the constraint names and the values are the constraint functions.
        3. metrics_titles_dict: A dictionary mapping the metric names to their titles.
        4. constraint_titles_dict: A dictionary mapping the constraint names to their titles.
        """
        preliminary_system_message = (
            "You are part of a query recommending process for a Pandas DataFrame. "
            "Your task is to create an interestingness metric and constraints for the queries that will be generated, with "
            "the goal of maximizing the interestingness of the queries. "
        )
        preliminary_user_message = (
            f"The DataFrame is named {self.df_name}. "
            f"The DataFrame has the following columns: {self.df.columns.tolist()}. "
            f"The column types are: {self.df.dtypes.to_dict()}. "
            f"The DataFrame has {self.df.shape[0]} rows and {self.df.shape[1]} columns. "
            f"The user requests are: {self.user_requests}. These should have the highest priority. "
            f"The queries generated as the initial recommendations are: {list(self.initial_recommendations)}. "
            f"Both the metric and the constraints should be in the format of a python expression on a dataframe called 'query_result', "
            f"that can be run directly using eval(). The constraints should return a boolean value, and the metric a float between 0 and 1. "
            f"Return the metric's code between <metric> and </metric>, and the constraints between <constraints> and </constraints> tags, "
            f"so they can be extracted programmatically. They should each be valid python functions. The constraints must not have "
            f"sub functions defined inside them, since we will be detecting each function inside the set by the 'def' keyword. "
            f"Also, all constraints should have a docstring that gives a short description of what they are, which will be provided "
            f"to the next step in the automated process. "
            f"There should also be a title for each metric, which should be returned between <title> and </title> tags, clearly stating what the metric is measuring."
            f"If there are multiple metrics, separate their titles with a newline, but make sure the titles are in order of the metrics. "
            f"There can be multiple metrics, and they can have as many lines of code as needed, though do try to keep it simple. "
            f"Only make sure each metric is a valid python function that starts with 'def'. There is no need for a docstring "
            f"in the metrics, since they will be used directly in the code. "
            f"There can be multiple constraints, but they should not be overly complex. Their goal is to maximize the interestingness of the queries when "
            f"composing your interestingness metric and our own custom metrics, which are based on the KS test and coefficient of variation. "
            f"(try not to have overlap of your metric with our metrics)."
            f"Remember that each tag should have all of the corresponding entries: if there are multiple metrics, they should all be inside "
            f"the same <metric> and </metric> tags, and likewise for the constraints and titles. "
            f"You also can not make any assumptions about what a query's result will contain, since this process may yield "
            f"many different queries, so you should write metrics and constraints that can always be applied to any DataFrame. "
            f"It is also very important that you properly normalize the metrics to be between 0 and 1, as otherwise scores may explore. "
            f"Also, if a metric is negative (lower is better), you should invert it to be positive (higher is better)."
        )
        preliminary_response = client(
            system_messages=[preliminary_system_message],
            user_messages=[preliminary_user_message],
        )
        # Extract the metric and constraints from the response
        metrics_code = self._extract_response(preliminary_response, "<metric>", "</metric>")
        constraints_code = self._extract_response(preliminary_response, "<constraints>", "</constraints>")
        titles = self._extract_response(preliminary_response, "<title>", "</title>")
        # Compile the metric code and constraints code
        if metrics_code is not None:
            titles = titles.split("\n") if titles is not None else None
            titles = [title.strip() for title in titles if title.strip()] if titles is not None else None
            metrics_code = metrics_code.split("def ")
            metrics_code = [f"def {metric.strip()}" for metric in metrics_code if metric.strip()]
            # Extract the function names as they would appear after exec
            metric_names = [metric.split(":", 1)[0].strip() for metric in metrics_code]
            metric_names = [metric_name.split("(")[0] for metric_name in metric_names]
            metric_names = [metric.replace("def ", "").replace(" ", "_") for metric in metric_names]
            # Create a dict for mapping between the metrics and their titles
            metrics_titles_dict = {
                metric: title for metric, title in zip(metric_names, titles)
            }
            # Compile the metrics
            metrics = {}
            for metric in metrics_code:
                try:
                    exec(metric, metrics)
                except Exception as e:
                    continue
            # Remove the __builtins__ from the metrics dictionary to avoid conflicts
            metrics.pop('__builtins__', None)
        else:
            metrics = {}
            metrics_titles_dict = {}
        if constraints_code is not None:
            constraints_list = constraints_code.split("def ")
            constraints_list = [f"def {constraint.strip()}" for constraint in constraints_list if constraint.strip()]
            # Extract the function names as they would appear after exec
            constraint_names = [constraint.split(":", 1)[0].strip() for constraint in constraints_list]
            constraint_names = [constraint_name.split("(")[0] for constraint_name in constraint_names]
            constraint_names = [constraint.replace("def ", "").replace(" ", "_") for constraint in constraint_names]
            constraints = {}
            # Compile the constraints
            for constraint in constraints_list:
                try:
                    exec(constraint, constraints)
                except Exception as e:
                    continue
            # Remove the __builtins__ from the constraints dictionary to avoid conflicts
            constraints.pop('__builtins__', None)
            constraints_titles = [constraint.split('"""', 2)[1].strip() for constraint in constraints_list]
            constraint_titles_dict = {
                constraint: title for constraint, title in zip(constraint_names, constraints_titles)
            }
        else:
            constraints = {}
            constraint_titles_dict = {}
        return metrics, constraints, metrics_titles_dict, constraint_titles_dict


    def _add_to_history(self, history: pd.DataFrame, applied_queries: dict[str, pd.DataFrame], metrics: dict, metric_titles_mapping: dict,
                        constraints: dict, constraint_titles_mapping: dict, summaries: list[str]) -> pd.DataFrame:
        """
        Adds new rows to the history DataFrame for the provided queries.
        Each query will have all of the metrics and constraints applied to it, and the results will be added to
        the history DataFrame.

        :param history: The history DataFrame to add the queries to.
        :param applied_queries: A dict where the keys are the queries and the values are the results of those queries.
        :param metrics: A dictionary of metrics to apply to the queries.
        :param metric_titles_mapping: A dictionary mapping the metric names to their titles.
        :param constraints: A dictionary of constraints to apply to the queries.
        :param constraint_titles_mapping: A dictionary mapping the constraint names to their titles.
        :param summaries: A list of summaries of the output of the actor's response, which will be used to provide context to the LLM in the next iteration.

        :return: The updated history DataFrame with the new queries added.
        """
        for i, (query, query_result) in enumerate(applied_queries.items()):
            query_dict = {}
            query_dict["query"] = query
            # Write the dict entries, with None as placeholders for the metrics and constraints.
            query_dict[f"Metric 1: {self.score_function_name}"] = None
            idx = 2
            for metric, metric_func in metrics.items():
                metric_title = metric_titles_mapping.get(metric, f"{metric}")
                query_dict[f"Metric {idx}: {metric_title}"] = None
                idx += 1
            idx = 1
            for constraint, constraint_func in constraints.items():
                constraint_title = constraint_titles_mapping.get(constraint, f"{constraint}")
                query_dict[f"Constraint {idx}: {constraint_title}"] = None
                idx += 1
            query_dict["Critic summary"] = summaries[i] if i < len(summaries) else None
            # If there is an error, we skip the query and keep the None values in the history.
            if query_result["error"]:
                query_dict["Error in query"] = True
            else:
                query_dict["Error in query"] = False
                # Otherwise, we apply the metrics and constraints to the query result.
                query_res = query_result["result"]
                _, score = self.score_function(query, query_res)
                query_dict[f"Metric 1: {self.score_function_name}"] = score
                # Apply the metrics to the query result
                idx = 2
                for metric, metric_func in metrics.items():
                    metric_title = metric_titles_mapping.get(metric, f"{metric}")
                    try:
                        metric_score = metric_func(query_res)
                        query_dict[f"Metric {idx}: {metric_title}"] = metric_score
                    except Exception as e:
                        query_dict[f"Metric {idx}: {metric_title}"] = f"Error: {str(e)}"
                    finally:
                        idx += 1
                # Apply the constraints to the query result
                idx = 1
                for constraint, constraint_func in constraints.items():
                    constraint_title = constraint_titles_mapping.get(constraint, f"{constraint}")
                    try:
                        constraint_result = constraint_func(query_res)
                        query_dict[f"Constraint {idx}: {constraint_title}"] = constraint_result
                    except Exception as e:
                        query_dict[f"Constraint {idx}: {constraint_title}"] = f"Error: {str(e)}"
                    finally:
                        idx += 1
                # Add the query dict to the history DataFrame
            history = pd.concat([history, pd.DataFrame([query_dict])], ignore_index=True)
        # Reset the index of the history DataFrame
        history = history.reset_index(drop=True)
        return history


    def _handle_list_string(self, list_str: str, line_start: str ="*", line_end: str = "\n") -> list[str]:
        """
        Handle a string that is a list of items, where each item is separated by a line_start and line_end.
        This is used to extract the queries from the LLM response.

        :param list_str: The string to handle.
        :param line_start: The string that indicates the start of each item in the list.
        :param line_end: The string that indicates the end of each item in the list.

        :return: A list of items extracted from the string.
        """
        if not list_str:
            return []
        items = list_str.split(line_end)
        items = [item.strip() for item in items if item.strip().startswith(line_start)]
        # The critic sometimes provides the constraints as a numbered list, despite the instructions to not do so.
        # So, we use a regex to remove leading numbers and periods from the constraints.
        pattern = re.compile(r'^\d+\.\s*')
        items = [pattern.sub('*', item) for item in items]
        items = [item[len(line_start):].strip() if item.startswith(line_start) else item.strip() for item in items]
        return items




    def do_llm_action(self) -> pd.Series | None | str:
        """
        Use LLMs to refine the queries.
        """
        client = Client()
        actor = LLMQueryRecommender(self.df, self.df_name)
        actor_responses = []
        critic_task = self._define_critic_task()
        actor_task = self._define_actor_task()
        # Save the current pandas options to reset them later
        display_max_rows = pd.get_option('display.max_rows')
        display_max_columns = pd.get_option('display.max_columns')
        display_width = pd.get_option('display.width')
        display_max_colwidth = pd.get_option('display.max_colwidth')
        # Disable the pandas options that limit the display of DataFrames, because that will truncate what the LLM sees
        # if we don't.
        pd.set_option('display.max_rows', None)  # Show all rows
        pd.set_option('display.max_columns', None)  # Show all columns
        pd.set_option('display.width', 0)  # No limit on the width of the display
        pd.set_option('display.max_colwidth', None)  # No limit on column width
        # Preliminary step: have a LLM create an interestingness metric and constraints for the queries.
        metrics, constraints, metrics_titles_dict, constraint_titles_dict = self._do_preliminary_LLM_call(client)
        print("Finished creating metrics and constraints.")
        # Create the history DataFrame
        history_columns = ["query", "Error in query", f"Metric 1: {self.score_function_name}"]
        idx = 2
        for metric in metrics.keys():
            metric_title = metrics_titles_dict.get(metric, f"{metric}")
            history_columns.append(f"Metric {idx}: {metric_title}")
            idx += 1
        idx = 1
        for constraint in constraints.keys():
            constraint_title = constraint_titles_dict.get(constraint, f"{constraint}")
            history_columns.append(f"Constraint {idx}: {constraint_title}")
            idx += 1
        history_columns.append("Critic summary")
        history_df = pd.DataFrame(columns=history_columns)
        applied_initial_recommendations = actor.do_follow_up_action(self.initial_recommendations)
        history_df = self._add_to_history(history_df, applied_initial_recommendations, metrics, metrics_titles_dict, constraints,
                                          constraint_titles_dict, summaries=["Initial recommendation"] * self.k)
        # Main loop of the process - iterate n times, each time getting critic feedback on existing recommendations,
        # then using that feedback to refine the recommendations using the actor.
        try:
            for i in range(self.n):
                print(f"Starting iteration {i + 1} / {self.n} of the refinement process.")
                critic_message = (f"This is iteration number {i + 1} / {self.n} of the refinement process. The current history is"
                                  f" {self._create_history_string(history_df, get_only_head=True, head_len=(history_df.shape[0] - self.k))}\n")
                critic_message += f"The current queries are: {self._create_history_string(history_df, get_only_tail=True, tail_len=self.k)}\n"
                critic_message += self._create_data_explanation() + self._create_critic_format_instructions()
                # Get the critic's response. We avoid using the assistant messages from the previous iterations,
                # because it can lead to too large of a context window.
                critic_response = client(
                    system_messages=[critic_task],
                    user_messages=[critic_message],
                )
                critic_long_response = self._extract_response(critic_response, "<critic>", "</critic>")
                critic_summaries = self._extract_response(critic_response, "<summary>", "</summary>")
                if critic_long_response is None or len(critic_long_response) < 1:
                    print("The critic did not provide a response. Stopping the process.")
                    break
                critic_long_responses = self._handle_list_string(critic_long_response, line_start="*", line_end="\n")
                critic_summaries = self._handle_list_string(critic_summaries, line_start="*", line_end="\n")
                # Replace the empty Critic summary of the current recommendations (aka the last k rows of the history DataFrame)
                # with the summaries provided by the critic.
                history_df.loc[history_df.shape[0] - self.k:, "Critic summary"] = critic_summaries
                actor_message = (f"This is iteration number {i + 1} / {self.n} of the refinement process. The current history is"
                                  f" {self._create_history_string(history_df, get_only_head=True, head_len=(history_df.shape[0] - self.k))}\n")
                actor_message += f"The queries generated in the previous iteration are: {self._create_history_string(history_df, get_only_tail=True, tail_len=self.k)}\n"
                actor_message += f"The critic's analysis of the queries is: {critic_long_responses}. \n"
                actor_message += self._create_data_explanation() + self._create_actor_format_instructions()
                # Get the actor's response
                actor_response = actor.do_llm_action(
                    system_messages=[actor_task],
                    user_messages=[actor_message],
                )
                if actor_response is None or len(actor_response) < 1:
                    break
                actor_responses.append(actor_response)
                # Apply the actor's response to the DataFrame and get the results
                applied_actor_response = actor.do_follow_up_action(actor_response)
                # Add the new queries to the history DataFrame
                history_df = self._add_to_history(history_df, applied_actor_response, metrics, metrics_titles_dict,
                                                    constraints, constraint_titles_dict, summaries=["Not seen by critic yet"] * len(applied_actor_response))
        except Exception as e:
            # If the LLM fails, we need to handle it gracefully.
            print(
                f"An error occurred while generating the recommendations: {e}. "
                f"Stopping the process and returning the current recommendations. "
            )
        # Once the process is complete, we need to get the final recommendations by their score.
        # These recommendations are the highest scoring queries produced throughout the process.
        fedex_col_name = f"Metric 1: {self.score_function_name}"
        """
        I debated what to go with - unified scores, or only the FEDEx scores.
        On the one hand, the unified scores can be more informative, since they take everything into account.
        On the other hand, FEDEx is the only one we actually know ahead of time (and is kind of the main goal), and the 
        functions and constraints put out by the LLM may be unreliable or even outright silly.
        I ended up going with just the FEDEx scores, since they are the most reliable and we know what they mean. 
        This relegates the other metrics and constraints to being there to guide the LLM to produce better queries, but not to actually be used in the final scoring.
        """
        # # Option 1 - use unified scores to sort the queries.
        # unified_scores = []
        # # Create a unified score for each query. This score is the average of the scores of all metrics,
        # # multiplied by the ratio of the number of constraints that are upheld by the query.
        # for i, row in history_df.iterrows():
        #     # Get the scores of the metrics
        #     metric_scores = [row[col] for col in history_df.columns if col.startswith("Metric")]
        #     # There can be strings in the score if errors occurred in the metric calculation,
        #     metric_scores = [score for score in metric_scores if isinstance(score, (int, float))]
        #     # Get the constraints that are upheld by the query
        #     constraints_upheld = [row[col] for col in history_df.columns if col.startswith("Constraint") and row[col] is True]
        #     # There can be strings in the constraints if errors occurred in the constraint calculation,
        #     constraints_upheld = [constraint for constraint in constraints_upheld if isinstance(constraint, bool)]
        #     # Calculate the unified score
        #     if len(metric_scores) > 0:
        #         unified_score = sum(metric_scores) / len(metric_scores)
        #         if len(constraints_upheld) > 0:
        #             unified_score *= len(constraints_upheld) / len(constraints)
        #         # If no constraints are upheld, we multiply the score by 0.05 to give it a small value, but not zero,
        #         else:
        #             unified_score *= 0.05
        #     else:
        #         unified_score = 0
        #     unified_scores.append(unified_score)
        # # Add the unified scores to the history DataFrame
        # history_df["Unified score"] = unified_scores
        # # Sort the history DataFrame by the unified score in descending order
        # history_df = history_df.sort_values(by="Unified score", ascending=False)

        # Option 2 - use only the FEDEx scores to sort the queries.
        # Sort the history DataFrame by the FEDEx score in descending order
        history_df = history_df.sort_values(by=fedex_col_name, ascending=False)


        if self.return_all_options:
            # If we want to return all options, we return the entire history DataFrame.
            recs = history_df
        else:
            # Otherwise, we return the top k queries from the history DataFrame.
            recs = history_df.head(self.k)
        # Reset pd options to original values
        pd.set_option('display.max_rows', display_max_rows)
        pd.set_option('display.max_columns', display_max_columns)
        pd.set_option('display.width', display_width)
        pd.set_option('display.max_colwidth', display_max_colwidth)
        recs = recs.rename(columns={fedex_col_name: "FEDEx Interestingness Score"})
        recs = recs.drop(columns=["Critic summary"], errors='ignore')
        return recs



