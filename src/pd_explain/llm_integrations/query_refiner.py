import re

import pandas as pd
from pandas import DataFrame, Series
from typing import Callable

from pd_explain.llm_integrations.llm_integration_interface import LLMIntegrationInterface
from pd_explain.llm_integrations.client import Client
from pd_explain.llm_integrations.llm_query_recommender import LLMQueryRecommender

class QueryRefiner(LLMIntegrationInterface):
    """
    QueryRefiner is based on the query refinement process in OmniTune (see paper "OmniTune: A universal framework for query refinement via LLMs"
    by Eldar Hacohen, Yuval Moskovitch, Amit Somech).
    We use this in the context of query recommenders to refine the queries that are generated by the query
    recommenders.
    """

    def __init__(self, df: DataFrame, df_name: str, recommendations: dict, score_function: Callable[[str, pd.DataFrame, list[str] | None], tuple[dict, float, dict]],
                 k=4, user_requests: str= None, n=2, return_all_options: bool = False):
        """
        Initialize the QueryRefiner with a DataFrame and a set of recommendations.

        :param df: The DataFrame to generate queries for.
        :param df_name: The name of the DataFrame.
        :param recommendations: The recommendations to refine.
        :param k: The number of queries to generate.
        :param user_requests: An optional list of user requests.
        :param n: The number of refining iterations to perform.
        :param return_all_options: If True, returns all queries generated instead of just the top k.
        """
        self.df = df
        self.df_name = df_name
        self.recommendations = recommendations
        self.k = k
        self.user_requests = user_requests if user_requests is not None else []
        self.n = n
        self.score_function = score_function
        self.return_all_options = return_all_options


    def _explain_scoring_method(self) -> str:
        explanation =  ("Filter and Join queries are scored using a KS test between the input and output distributions, and GroupBy queries are scored using the coefficient of variation of their output. "
                       "Each column has an individual score, while the final score is the geometric average of the 4 highest scoring columns after transforming the scores to be between 0 and 1. "
                        "A higher score is better. ")
        return explanation


    def _define_critic_task(self) -> str:
        """
        Define the critic task for the LLM.
        """
        critic_task = ("You are a critic in an actor-critic framework. "
                       "This framework is used to refine queries for a Pandas DataFrame query recommender. "
                       "You will be provided with a description of both the input and output of multiple queries, as well as the"
                       " queries themselves and their score + the score of each column that makes up the score. "
                       "If there are constraints on the queries, you will also be provided with them and whether they are upheld or not. "
                       "You will also be provided with historic queries from this process. "
                       "Your task is to analyze and explain why the score is what it is, which the actor will then use to refine the query. Explicitly state the good and bad parts (if there are any) of the query. "
                       "Alternatively, if the query has an error in it (error message or score of 0), suggest a fix for it. "
                       "This analysis should be short, 1 sentence per query."
                       "Additionally, suggest constraints on the query that would make it more interesting. "
                       "Constraints must be given as a python expression on a dataframe called 'query_result', which can be evaluated to "
                       "a boolean value. If you would like to not add a constraint, write None instead in the position corresponding "
                       "to the query. However, not adding constraints is not recommended and should only be done if there is no way to add a meaningful constraint. "
                       "The constraints should have 1 clause at most. Avoid generating similar constraints to the ones already generated. "
                       "Avoid constraints that are a simple filter, such as query_result['column'] > 0. Those are not interesting constraints. "
                       "Make constraints more specific as the iterations go on. Make sure the columns in the constraints match the column names you are provided. "
                       f"{self._explain_scoring_method()}")

        return critic_task


    def _define_actor_task(self) -> str:
        """
        Define the actor task for the LLM.
        """
        actor_task = ("You are an actor in an actor-critic framework. "
                      "This framework is used to refine queries for a Pandas DataFrame query recommender. "
                      "You will be provided with a description of both the input and output of the queries, as well as the "
                      "queries themselves and their score + the score of each column that makes up the score. "
                      "If there are constraints on the queries, you will also be provided with them and whether they are upheld or not by the current queries. "
                      "You will also be provided with the critic's analysis of the query, and historic queries from this process. "
                      "Your task is to use the critic's analysis to refine the query to make a query that is more interesting, or outright replace the query with a new one if there is no way to refine or fix it. "
                      "If the critic has added constraints, you must make sure that the new query upholds them. Also make sure the "
                      "The critic's constraints are in the format of a python expression on a dataframe called 'query_result' - you only need to understand "
                      "the constraint, and not the code itself. Never add the constraints to the query itself, your job is to make sure the query "
                      "upholds the constraints by itself and not because you explicitly added them to the query. "
                      "If a query did not uphold a constraint in the previous iteration, you must make sure that the new query upholds it. "
                      "You must also replace the query if it uses the head or tail methods, as these are not interesting queries, even if they have a high score. "
                      "Avoid generating queries that are very similar to the ones already in the history or to one another. "
                      f"{self._explain_scoring_method()} .")

        return actor_task


    def _create_data_explanation(self) -> str:
        """
        Create an explanation for the LLM, explaining the context of the DataFrame.
        This includes the columns, their types, and any other relevant information.
        """
        data_explanation = f"The DataFrame is named {self.df_name}. "
        data_explanation += f"The DataFrame has the following columns: {self.df.columns.tolist()}. "
        data_explanation += (f"The column types are: {self.df.dtypes.to_dict()}. "
                             f"The DataFrame has {self.df.shape[0]} rows and {self.df.shape[1]} columns. ")
        # This line can take up too much of the context window, depending on the size of the DataFrame.
        # data_explanation += f"Using df.describe() we get the following statistics: {self.df.describe().to_dict()}."
        if self.user_requests:
            data_explanation += f"The user requests are: {self.user_requests}. These should have the highest priority. "

        return data_explanation


    def _create_query_list(self, constraint_list: list[str] = None) -> str:
        queries = list(self.recommendations.keys())
        query_scores = [self.recommendations[query]['score'] for query in queries]
        query_columns_scores = [self.recommendations[query]['score_dict'] for query in queries]
        query_results = [self.recommendations[query]['query_result'] for query in queries]
        query_list = ""
        query_list += f"The currently recommended queries are: {queries} "
        query_list += f"Their scores are (in order): {query_scores}. "
        query_list += f"Their column scores are (in order): {query_columns_scores}. "
        # The below line is commented out because it can take up a lot of the context window, which can lead to errors.
        # query_list += f"Their result descriptions are (in order): {[query_result.describe().to_dict() for query_result in query_results]}. "
        query_list += f"The number of rows in the result is (in order): {[q.shape[0] if (isinstance(q, DataFrame) or isinstance(q, Series)) else 0 for q  in query_results]}. \n"
        if constraint_list is not None and len(constraint_list) > 0:
            constraint_list_str = ""
            for i, constraint in enumerate(constraint_list):
                if constraint is not None:
                    constraint_list_str += f"Query {i + 1}: {constraint} \n"
            query_list += (f"The following are the constraints the queries are supposed to uphold. None means there were no "
                           f"constraints added that round. The number denotes the query each list of constraints corresponds to. "
                           f"{constraint_list_str}\n. ")
        return query_list


    def _create_critic_format_instructions(self) -> str:
        """
        Explain the expected format of the output to the LLM.
        """
        format_instructions = ("The output should be 2 lists, both denoted with a * and a newline for each entry. "
                               "The first list should be the critic's analysis of the queries, one for each input query. In this list, "
                               "you should provide short explanations of the score of each query, and if there are any errors in the query, "
                               "you should suggest a fix for it. "
                               "The second list should be the constraints on the queries, one for each input query. "
                               "The constraints should be the python code and ONLY the python code, as it will be evaluated and run directly. "
                               "Any other text will cause an exception. "
                               "The first list should be enclosed by <scores> and </scores>, and the second list should be enclosed by <constraints> and </constraints>. "
                               "Not adhering to this format will cause the system to fail. "
                               "You must never, under no circumstances, repeat the same constraints as those provided to you. "
                               "You are adding constraints to the list provided to you, repeating them is meaningless. "
                               "Constraints must use exactly the same column names as the ones provided to you, without any changes. "
                               f"If there are less than {self.k} queries, add None to the corresponding positions in both the score analysis "
                               f"list and the constraints list, so there are always {self.k} entries. ")

        return format_instructions


    def _create_actor_format_instructions(self) -> str:
        """
        Explain the expected format of the output to the LLM.
        """
        format_instructions = ("The output should be a list of queries, one for each input query. "
                               "The list should be denoted with a * and a newline for each query. Absolutely never combine multiple queries into one line, as you will cause the system to fail. "
                               "The list must be surrounded by <recs> and </recs> at the beginning and end respectively. Failure to do "
                               "this will cause the system to fail. ")

        return format_instructions



    def do_llm_action(self) -> pd.Series | None | str:
        """
        Use LLMs to refine the queries.
        """
        client = Client()
        actor = LLMQueryRecommender(self.df, self.df_name)
        critic_user_messages = []
        critic_responses = []
        actor_user_messages = []
        actor_responses = []
        history = pd.DataFrame(columns=["query", "score", "explanation"])
        critic_task = self._define_critic_task()
        actor_task = self._define_actor_task()
        # Save the current pandas options to reset them later
        display_max_rows = pd.get_option('display.max_rows')
        display_max_columns = pd.get_option('display.max_columns')
        display_width = pd.get_option('display.width')
        display_max_colwidth = pd.get_option('display.max_colwidth')
        constraints = [[] for _ in range(self.k)]
        # Disable the pandas options that limit the display of DataFrames, because that will truncate what the LLM sees
        # if we don't.
        pd.set_option('display.max_rows', None)  # Show all rows
        pd.set_option('display.max_columns', None)  # Show all columns
        pd.set_option('display.width', 0)  # No limit on the width of the display
        pd.set_option('display.max_colwidth', None)  # No limit on column width
        # Main loop of the process - iterate n times, each time getting critic feedback on existing recommendations,
        # then using that feedback to refine the recommendations using the actor.
        recommendations = None
        try:
            for i in range(self.n):
                previous_constraints = []
                if recommendations is not None:
                    for query in recommendations.keys():
                        constraint_string = ""
                        for constraint in recommendations[query]['constraints']:
                            constraint_string += f"{constraint}, upheld: {recommendations[query]['constraints'][constraint]}, "
                        constraint_string = constraint_string[:-2]  # Remove the last comma and space
                        previous_constraints.append(constraint_string)
                else:
                    previous_constraints = ["None" for _ in range(self.k)]
                critic_message = ""
                if i >= 1:
                    critic_message += f"This is iteration number {i + 1} of the refinement process. The previous iteration history is (in CSV format): {history.to_csv()}\n"
                critic_message += self._create_data_explanation() + self._create_query_list(previous_constraints) + self._create_critic_format_instructions()
                critic_user_messages.append(critic_message)
                # Get the critic's response. We avoid using the assistant messages from the previous iterations,
                # because it can lead to too large of a context window.
                critic_response = client(
                    system_messages=[critic_task],
                    user_messages=[critic_message],
                )
                critic_scores = self._extract_response(critic_response, "<scores>", "</scores>")
                if critic_scores is None:
                    break
                critic_constraints = self._extract_response(critic_response, "<constraints>", "</constraints>")
                if critic_constraints is None:
                    break
                # The critic sometimes provides the constraints as a numbered list, despite the instructions to not do so.
                # So, we use a regex to remove leading numbers and periods from the constraints.
                pattern = r"\d+\.\s*|-\s*"
                critic_constraints = re.sub(pattern, "*", critic_constraints)
                # Break up the scores and constraints into lists and clean them up
                critic_scores = critic_scores.split("\n")
                critic_constraints = critic_constraints.split("\n")
                critic_scores = [score.replace("*", "").strip() for score in critic_scores if len(score) > 1]
                critic_constraints = [constraint.replace("*", "").strip() for constraint in critic_constraints if len(constraint) > 1]
                # Sometimes, empty strings happen because of the above split, so we need to filter them out.
                critic_scores = [score for score in critic_scores if len(score) > 1]
                critic_constraints = [constraint for constraint in critic_constraints if len(constraint) > 1]
                for j, constraint in enumerate(critic_constraints):
                    constraints[j].append(constraint)
                critic_response = ""
                for score, constraint in zip(critic_scores, critic_constraints):
                    # The critic's response is a list of scores and constraints, one for each query.
                    # We need to format it so that it is easy to read.
                    if len(score) > 0:
                        critic_response += f"Score explanations: {score}, "
                    if len(constraint) > 0:
                        critic_response += f"Constraint recommendation: {constraint}"
                    critic_response += "\n"
                critic_responses.append(critic_response)
                # Likewise with the actor, the first iteration includes a full explanation of the source and target dataframes,
                # while the rest of the iterations only include the query list and the actor format instructions.
                actor_message = ""
                if i >= 1:
                    actor_message += f"This is iteration number {i + 1} of the refinement process. The previous iteration history is (in CSV format): {history.to_csv()}\n"
                actor_message += self._create_data_explanation() + self._create_query_list() + self._create_actor_format_instructions()
                if len(self.recommendations) < self.k:
                    actor_message += (f"There should have been {self.k} queries, but due to errors, there are only {len(self.recommendations)} queries. "
                                      f"Please make sure to generate more queries using the information provided. ")
                actor_message += f"The critic's analysis is: {critic_response}. "
                if not history.empty:
                    actor_message += f"The history of this process is (in order from beginning to end): {history}. "
                actor_user_messages.append(actor_message)
                # Get the actor's response
                actor_response = actor.do_llm_action(
                    system_messages=[actor_task],
                    user_messages=[actor_message],
                )
                if actor_response is None or len(actor_response) < 1:
                    break
                actor_responses.append(actor_response)
                # Apply the actor's response to the DataFrame and get the results
                applied_actor_response = actor.do_follow_up_action(actor_response)
                recommendations = {}
                # Score the queries
                for j, zipped in enumerate(applied_actor_response.items()):
                    query, query_result = zipped
                    # Score the query
                    scores, score, constraints_dict = self.score_function(query, query_result, constraints[j])
                    recommendations[query] = {
                        "query_result": query_result["result"],
                        "score_dict": scores,
                        "score": score,
                        "constraints": constraints_dict,
                    }
                # Add the previous queries to the history
                previous_queries = list(self.recommendations.keys())
                previous_scores = [self.recommendations[query]['score'] for query in previous_queries]
                previous_critics = [response.replace("*", "").strip() for response in critic_response.split("\n") if response]
                # Sometimes, empty strings happen because of the above split, so we need to filter them out.
                previous_critics = [critic for critic in previous_critics if len(critic) > 1]
                history = pd.concat([history, pd.DataFrame({
                    "query": previous_queries,
                    "score": previous_scores,
                    "explanation": previous_critics,
                })], ignore_index=True)
                self.recommendations = recommendations
                print(f"Finished iteration {i + 1} of the refinement process. ")
        except Exception as e:
            # If the LLM fails, we need to handle it gracefully.
            print(
                f"An error occurred while generating the recommendations: {e}. "
                f"Stopping the process and returning the current recommendations. "
            )
        # Once the process is complete, we need to get the final recommendations by their score.
        # These recommendations are the highest scoring queries produced throughout the process.
        history = history[["query", "score"]]
        recs = pd.DataFrame(columns=["query", "score"])
        for query, query_dict in self.recommendations.items():
            recs = pd.concat([recs, pd.DataFrame({
                "query": [query],
                "score": [query_dict["score"]]
            })], ignore_index=True)
        recs = pd.concat([recs, history], ignore_index=True)
        # If there are any duplicate queries, we only keep the last one.
        recs = recs.drop_duplicates(subset=["query"], keep="last")
        # Keep the k highest scoring queries.
        recs = recs.sort_values(by=["score"], ascending=False)
        recs = recs.reset_index(drop=True)
        print(f"Number of final recommendations: {len(recs)}")
        if not self.return_all_options:
            # If we are not returning all options, we only keep the top k recommendations.
            recs = recs.head(self.k)
        # Reset pd options to original values
        pd.set_option('display.max_rows', display_max_rows)
        pd.set_option('display.max_columns', display_max_columns)
        pd.set_option('display.width', display_width)
        pd.set_option('display.max_colwidth', display_max_colwidth)
        return recs



